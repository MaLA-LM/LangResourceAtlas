This page shows the statistics of ðŸ¤—[MaLA-LM/mala-monolingual-dedup](https://huggingface.co/datasets/MaLA-LM/mala-monolingual-dedup) and its language resource groups categorized by the white-space separated token counts. 

Limitations:
- MaLA corpus is designed for language adaptation or continual pre-training of LLMs and focuses on collecting low-resources. Thus, some lower resource languages can be categorized into higher resource. 
- White-space tokenization fails to calculate the token counts correctly in some languages such as Chinese. 

---
The **MaLA Corpus** (Massive Language Adaptation) is a comprehensive, multilingual dataset designed to support the continual pre-training of large language models. It covers **939 languages** and consists of over **74 billion tokens**, making it one of the largest datasets of its kind. With a focus on improving the representation of low-resource languages, the MaLA Corpus is a critical resource for advancing multilingual models, particularly those aimed at serving underrepresented languages.

- Project page: https://mala-lm.github.io/emma-500
- Paper: https://arxiv.org/abs/2409.17892

